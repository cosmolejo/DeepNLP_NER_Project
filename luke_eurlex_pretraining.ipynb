{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dyWex1OT0tY"
      },
      "source": [
        "# further pretraining LUKE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R--t4Sib3cOL",
        "outputId": "cd2f01e2-9ac5-4e66-f164-e212faccd133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_cmLdFPzvwX",
        "outputId": "75dba088-c873-46d6-b784-ec641b7ad25c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksPLVvavzvuB"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "from transformers import AdamW\n",
        "import datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU3C_qzwzwte"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Load your custom dataset\n",
        "dataset_name = \"eurlex\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "# Check the type of the dataset\n",
        "print(type(train_dataset))\n",
        "print(type(test_dataset))\n",
        ",,,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLXfhjo19Q_e"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_from_disk(\"/content/drive/MyDrive/eurlex_dataset/eurlex_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVWiN9pI95i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd3279f-9745-4f7a-e471-bcff0eca3e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['celex_id', 'title', 'text', 'eurovoc_concepts'],\n",
            "    num_rows: 45000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnQBcDTFf8En"
      },
      "outputs": [],
      "source": [
        "# List to store dictionaries for each row\n",
        "dataset_list = []\n",
        "\n",
        "# Iterate over the dataset\n",
        "for example in dataset:\n",
        "    # Create a dictionary for the current row\n",
        "    row_dict = {\n",
        "        'celex_id': example['celex_id'],\n",
        "        'title': example['title'],\n",
        "        'text': example['text'],\n",
        "        'eurovoc_concepts': example['eurovoc_concepts']\n",
        "    }\n",
        "\n",
        "    # Append the dictionary to the list\n",
        "    dataset_list.append(row_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cszu3MAKf_0Y",
        "outputId": "22de5223-bbae-4be4-9cae-27d0520ad53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45000\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86flHrF6gCpx",
        "outputId": "28535a7b-dd4c-4c1f-c5ba-4c3dacf85cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in part 1: 22500\n",
            "Number of rows in part 2: 22500\n"
          ]
        }
      ],
      "source": [
        "# Calculate the split index\n",
        "split_index = len(dataset_list) // 2\n",
        "\n",
        "# Divide the dataset list into two equal parts\n",
        "dataset_part1 = dataset_list[:split_index]\n",
        "dataset_part2 = dataset_list[split_index:]\n",
        "\n",
        "# Print the number of rows in each part\n",
        "print(\"Number of rows in part 1:\", len(dataset_part1))\n",
        "print(\"Number of rows in part 2:\", len(dataset_part2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kbf0HrUgINP",
        "outputId": "c669fbac-5199-4484-9c27-cebad3604e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22500\n"
          ]
        }
      ],
      "source": [
        "print(split_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "107oLu598zJ-"
      },
      "outputs": [],
      "source": [
        "from transformers import LukeTokenizer, LukeForMaskedLM, LukeConfig\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TSQRy-x8zG2",
        "outputId": "da03094d-7cb1-4004-f1d4-fcd2ddda54ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# Create a LukeTokenizer\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onMkt44TSJ1s",
        "outputId": "0c100d00-584b-4873-bb33-732872404a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenizer has a 'mask_token_id': 50264\n"
          ]
        }
      ],
      "source": [
        "from transformers import LukeTokenizer\n",
        "\n",
        "# Load LukeTokenizer\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
        "\n",
        "# Check if the tokenizer has a \"mask_token_id\"\n",
        "if hasattr(tokenizer, \"mask_token_id\"):\n",
        "    print(\"The tokenizer has a 'mask_token_id':\", tokenizer.mask_token_id)\n",
        "else:\n",
        "    print(\"The tokenizer does not have a 'mask_token_id'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De0hZkIT9nuI",
        "outputId": "06726cbc-e02d-4142-9ec3-9f06f85bdd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'celex_id': '32006R0427', 'title': 'Commission Regulation (EC) No\\xa0427/2006 of  15 March 2006  establishing the standard import values for determining the entry price of certain fruit and vegetables\\n', 'text': '16.3.2006 EN Official Journal of the European Union L 79/2\\nCOMMISSION REGULATION (EC) No 427/2006\\nof 15 March 2006\\nestablishing the standard import values for determining the entry price of certain fruit and vegetables\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES\\n,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Commission Regulation (EC) No 3223/94 of 21 December 1994 on detailed rules for the application of the import arrangements for fruit and vegetables\\xa0(1), and in particular Article 4(1) thereof,\\nWhereas:\\n(1) Regulation (EC) No 3223/94 lays down, pursuant to the outcome of the Uruguay Round multilateral trade negotiations, the criteria whereby the Commission fixes the standard values for imports from third countries, in respect of the products and periods stipulated in the Annex thereto.\\n(2) In compliance with the above criteria, the standard import values must be fixed at the levels set out in the Annex to this Regulation,\\nThe standard import values referred to in Article 4 of Regulation (EC) No 3223/94 shall be fixed as indicated in the Annex hereto.\\nThis Regulation shall enter into force on 16 March 2006.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.', 'eurovoc_concepts': ['1118', '1605', '2635', '5231', '693']}\n",
            "{'celex_id': '31990D0111', 'title': '90/111/EEC: Commission Decision of 19 February 1990 amending Decision 84/248/EEC authorizing the Federal Republic of Germany to adopt, when introducing into its territory plants or plant products, special plant health provisions for home-grown production of certain fruit plants intended for planting (Only the German text is authentic)\\n', 'text': \"COMMISSION  DECISION\\nof 19 February 1990\\namending Decision 84/248/EEC authorizing the Federal Republic of Germany to adopt, when introducing into its territory plants or plant products, special plant health provisions for home-grown production of certain fruit plants intended for planting\\n(Only the German text is authentic)\\n(90/111/EEC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES\\n,\\nHaving regard to the Treaty establishing the European Economic Community,\\nHaving regard to Council Directive 77/93/EEC of 21 December 1976 on protective measures against the introduction into the Member States of organisms harmful to plants or plant products (1), as last amended by Directive 89/439/EEC (2), and in particular Article 18 (2) thereof,\\nWhereas, pursuant to Article 11 (1) of Directive 77/93/EEC, plants, plant products or other objects are not subject, at the time of their introduction into the territory of a Member State from another Member State, to prohibitions or restrictions relating to plant health measures, except where provided for in that Directive;\\nWhereas, pursuant to Article 18 (2) of Directive 77/93/EEC, the Member States may be authorized to adopt, when introducing into their territory plants or plant products, special health provisions in so far as such measures are also laid down for home-grown production;\\nWhereas the Federal Republic of Germany by the 'Verordnung zur Bekaempfung von Viruskrankheiten im Obstbau' of 26 July 1978 (3), as last amended by 'Verordnung' of 1 December 1989 (4), has introduced special health measures for the marketing of certain fruit plants intended for planting;\\nWhereas by Decision 84/248/EEC (5), as last amended by Decision 89/355/EEC (6), the Commission authorized the Federal Republic of Germany to apply the measures also to products introduced into its territory from other Member States;\\nWhereas this authorization was granted on a temporary basis for a period expiring on 31 December 1989, but subject to a possible extension, pending the definition of a Community certification scheme for fruit-plant propagating material;\\nWhereas the Community certification scheme has not yet been defined; whereas the other circumstances which justified this authorization have not changed;\\nWhereas therefore the authorization should be extended for a further period;\\nWhereas the measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Plant Health,\\nDecision 84/248/EEC is hereby amended as follows:\\n1. In Article 2, '31 December 1989' is replaced by '31 December 1990'.\\n2. The Annex is replaced by the Annex hereto.\\nThis Decision is addressed to the Federal Republic of Germany.\", 'eurovoc_concepts': ['1763', '191', '2409', '322']}\n",
            "{'celex_id': '31977R1805', 'title': 'Commission Regulation (EEC) No 1805/77 of 4 August 1977 laying down, in respect of the beef and veal sector, special rules for the application of Regulation (EEC) No 1055/77 on the storage and movement of products bought in by an intervention agency\\n', 'text': '5.8.1977 EN Official Journal of the European Communities L 198/19\\nCOMMISSION REGULATION (EEC) No 1805/77\\nof 4 August 1977\\nlaying down, in respect of the beef and veal sector, special rules for the application of Regulation (EEC) No 1055/77 on the storage and movement of products bought in by an intervention agency\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES\\n,\\nHaving regard to the Treaty establishing the European Economic Community,\\nHaving regard to Council Regulation (EEC) No 1055/77 of 17 May 1977 on the storage and movement of products bought in by an intervention agency\\xa0(1), and in particular Article 4 (6) thereof,\\nWhereas Commission Regulation (EEC) No 1722/77 of 28 July 1977\\xa0(2) laid down common rules for the application of Regulation (EEC) No 1055/77 on the storage and movement of products bought in by an intervention agency; whereas special rules for the application of Regulation (EEC) No 1055/77 should be adopted in respect of the beef and veal sector;\\nWhereas Article 3 (2) of Regulation (EEC) No 1055/77 lays down that products which are held by an intervention agency outside the territory of the Member State within whose jurisdiction it falls and which are not brought back into that Member State are to be disposed of at the prices and subject to the conditions laid down or to be laid down for the place of storage; whereas, where the place of storage is situated on the territory of another Member State, it is necessary to take into account, when calculating the selling price in the case of a sale at a price fixed in advance and when fixing the minimum price in the case of a sale by tender, any monetary or accession compensatory amounts levied or granted in respect of trade in an identical product between the Member States in question;\\nWhereas, where the place of storage is situated outside the Community, account should be taken of any refund granted;\\nWhereas it is necessary to waive certain provisions of Article 9 (3) of Commission Regulation (EEC) No 193/75 of 17 January 1975 laying down common detailed rules for the application of the system of import and export licences and advance fixing certificates for agricultural products\\xa0(3), as last amended by Regulation (EEC) No 1470/77\\xa0(4);\\nWhereas, for purposes of the provision of proof that the products stored in a non-member country have been placed in free circulation, the provisions of Article 11 of Commission Regulation (EEC) No 192/75 of 17 January 1975 laying down detailed rules for the application of export refunds in respect of agricultural products\\xa0(5), as last amended by Regulation (EEC) No 1633/77\\xa0(6), should be applied;\\nWhereas, to simplify administration, provision should be made for the selling price of the products held by an intervention agency and stored outside the territory of the Member State within whose jurisdiction that agency falls to be paid in the currency of that Member State;\\nWhereas, with a view to harmonization, all products stored outside the territory of the Member State within whose jurisdiction the intervention agency which holds those products falls should be subject to the provisions of this Regulation;\\nWhereas the measures provided for in this Regulation are in accordance with the opinion of the Management Committee for Beef and Veal,\\n1.\\xa0\\xa0\\xa0Where beef products held by an intervention agency and stored on the territory of a Member State other than the Member State within whose jurisdiction that agency falls are sold at a price fixed in advance, the selling price of those products shall correspond to the selling price of an identical product in the Member State within whose jurisdiction the intervention agency falls, minus or plus any monetary or accession compensatory amounts applicable in trade between that Member State and the Member State on whose territory the product is stored.\\n2.\\xa0\\xa0\\xa0The rate to be used to calculate the monetary and accession compensatory amounts referred to in paragraph 1 shall be that applicable on the day on which the contract of sale is concluded.\\n1.\\xa0\\xa0\\xa0Where the products held by an intervention agency and stored in a non-member country are sold at a price fixed in advance, the selling price of the product shall correspond to the selling price of an identical product in the Member State within whose jurisdiction the intervention agency falls, minus the amount of the lowest refund and minus or plus any monetary or accession compensatory amounts which would be applicable if the identical product was exported to non-member countries.\\n2.\\xa0\\xa0\\xa0The rate to be used to calculate the amount of the lowest refund and the monetary and accession compensatory amounts referred to in paragraph 1 shall be that applicable on the day on which the contract of sale is concluded.\\nHowever, if on the day on which the contract is concluded the purchaser produces a certificate of advance fixing of the refund, the rate to be used to calculate the amount of the lowest refund shall be that applicable on the day of the advance fixing.\\nWhere the preceding subparagraph applies, the day on which the contract of sale is concluded shall be considered as the day on which customs export formalities are completed.\\nNotwithstanding Article 9 (3) of Regulation (EEC) No 193/75, the intervention agency shall make the entries on and endorse the certificate.\\n3.\\xa0\\xa0\\xa0If the purchaser provides proof within six months from the day on which the contract of sale was concluded that the products purchased were placed in free circulation in the non-member country where they were stored or in another non-member country, the difference between the amount of the refund applicable for the non-member country where the products were placed in free circulation and the amount of the lowest refund shall be deducted from the selling price as specified in paragraph 1.\\nThe proof referred to in the preceding subparagraph shall be provided in accordance with Article 11 of Regulation (EEC) No 192/75.\\nWhere beef products held by an intervention agency and stored on the territory of one or more Member States other than the Member State within whose jurisdiction that agency falls, or in a non-member country, are sold by tender, a minimum selling price shall be fixed, in accordance with the procedure laid down in Article 27 of Regulation (EEC) No 805/68, for each Member State or non-member country where the products are stored and for each product.\\nThe selling prices of the products covered by this Regulation shall be paid in the currency of the Member State within whose jurisdiction the intervention agency which holds the products falls.\\nFor the purposes of this Regulation, the day on which the intervention agency accepts the request for a contract shall be considered as the day on which the contract of sale is concluded.\\nThis Regulation shall enter into force on the day of its publication in the Official Journal of the European Communities.\\nIt shall apply to products stored outside the territory of the Member State within whose jurisdiction the intervention agency which holds those products falls, irrespective of the date on which storage thereof commenced.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.', 'eurovoc_concepts': ['1988', '20', '2670', '3170', '4294', '4682']}\n",
            "{'celex_id': '31984D0384', 'title': '84/384/EEC: Council Decision of 23 July 1984 concerning a contribution to the European Coal and Steel Community from the general budget of the European Communities\\n', 'text': \"COUNCIL  DECISION\\nof 23 July 1984\\nconcerning a contribution to the European Coal and Steel Community from the general budget of the European Communities\\n(84/384/EEC)\\nTHE COUNCIL OF THE EUROPEAN\\nCOMMUNITIES\\n,\\nHaving regard to the Treaty establishing the European Economic Community, and in particular Article 235 thereof,\\nHaving regard to the proposal from the Commission,\\nHaving regard to the opinion of the European Parliament (1),\\nHaving regard to the opinion of the Economic and Social Committee (2),\\nWhereas, at its meeting on 22 May 1984, the Council confirmed that solid fuels are an essential element of Community energy strategy;\\nWhereas the Community coal industry is being restructured and modernized;\\nWhereas this restructuring and modernization will inevitably lead to exceptional job losses; whereas support measures should be envisaged to alleviate the consequences of these job losses; whereas it is therefore necessary to apply Article 56 (2) (b) of the Treaty establishing the European Coal and Steel Community;\\nWhereas, in the present circumstances, the resources provided for by the ECSC Treaty are not sufficient to finance these measures;\\nWhereas the secondary effects of this situation, if not remedied, would be liable to aggravate considerably the Community's general employment situation and to impair the harmonious development of economic activities, and this would undermine the achievement of one of the Community's main objectives;\\nWhereas the social measures envisaged will be based, on the one hand, on the number of jobs lost over a given period and, on the other hand, on the level of social expenditure undertaken by each Member State in respect of each job lost;\\nWhereas an appropriation of 60 million ECU has been entered in Chapter 10 0 of the general budget of the European Communities for 1984,\\nAn exceptional contribution of 60 million ECU is hereby granted to the European Coal and Steel Community, for the financial year 1984, out of the general budget of the European Communities for the same financial year, to ensure Community financing of social measures in favour of the coal industry, in accordance with the relevant Articles of the ECSC Treaty.\", 'eurovoc_concepts': ['3350', '5051', '5327']}\n",
            "{'celex_id': '32003R0081', 'title': 'Commission Regulation (EC) No 81/2003 of 17 January 2003 fixing the minimum selling prices for butter and the maximum aid for cream, butter and concentrated butter for the 111th individual invitation to tender under the standing invitation to tender provided for in Regulation (EC) No 2571/97\\n', 'text': 'Commission Regulation (EC) No 81/2003\\nof 17 January 2003\\nfixing the minimum selling prices for butter and the maximum aid for cream, butter and concentrated butter for the 111th individual invitation to tender under the standing invitation to tender provided for in Regulation (EC) No 2571/97\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES\\n,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Council Regulation (EC) No 1255/1999 of 17 May 1999 on the common organisation of the market in milk and milk products(1), as last amended by Commission Regulation (EC) No 509/2002(2), and in particular Article 10 thereof,\\nWhereas:\\n(1) The intervention agencies are, pursuant to Commission Regulation (EC) No 2571/97 of 15 December 1997 on the sale of butter at reduced prices and the granting of aid for cream, butter and concentrated butter for use in the manufacture of pastry products, ice-cream and other foodstuffs(3), as last amended by Regulation (EC) No 635/2000(4), to sell by invitation to tender certain quantities of butter that they hold and to grant aid for cream, butter and concentrated butter. Article 18 of that Regulation stipulates that in the light of the tenders received in response to each individual invitation to tender a minimum selling price shall be fixed for butter and maximum aid shall be fixed for cream, butter and concentrated butter. It is further stipulated that the price or aid may vary according to the intended use of the butter, its fat content and the incorporation procedure, and that a decision may also be taken to make no award in response to the tenders submitted. The amount(s) of the processing securities must be fixed accordingly.\\n(2) The measures provided for in this Regulation are in accordance with the opinion of the Management Committee for Milk and Milk Products,\\nThe minimum selling prices and the maximum aid and processing securities applying for the 111th individual invitation to tender, under the standing invitation to tender provided for in Regulation (EC) No 2571/97, shall be fixed as indicated in the Annex hereto.\\nThis Regulation shall enter into force on 18 January 2003.\\nThis Regulation shall be binding in its entirety and directly applicable in all Member States.', 'eurovoc_concepts': ['20', '2664', '2681', '2741', '3003', '301', '4498']}\n"
          ]
        }
      ],
      "source": [
        "# Print the first few samples in the train_dataset\n",
        "#dataset_part1\n",
        "#dataset_part2\n",
        "\n",
        "for i in range(5):\n",
        "    print(dataset_part2[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-ZA1e8X8y-f"
      },
      "outputs": [],
      "source": [
        "# Access the text data from the dataset\n",
        "texts_train = [sample[\"text\"] for sample in dataset_part2]\n",
        "\n",
        "\n",
        "# Tokenize your text data\n",
        "tokenized_data = tokenizer(texts_train, padding=True, truncation=True, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05Q2iecUNIwD",
        "outputId": "9751ec2a-c29c-4ad9-bbb5-30a6c096b3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1 - Input IDs: tensor([    0,  1549,     4,   246,     4, 32701, 13245, 13485,  3642,     9,\n",
            "            5,   796,  1332,   226,  7589,    73,   176, 50118, 10370,   448,\n",
            "        41336, 26747,  6597,  6034,    36,  3586,    43,   440, 41518,    73,\n",
            "        32701, 50118,  1116,   379,   494,  3503, 50118, 30248,   154,     5,\n",
            "         2526,  6595,  3266,    13, 13684,     5,  3555,   425,     9,  1402,\n",
            "         6231,     8,  8942, 50118, 13354, 31299, 41336,  3243,  1941, 10353,\n",
            "        23075,  1889, 37275, 26824, 50118,     6, 50118, 15852,  6203,     7,\n",
            "            5, 20704, 10584,     5,   796,  2573,     6, 50118, 15852,  6203,\n",
            "            7,  1463, 18912,    36,  3586,    43,   440,   155, 27996,    73,\n",
            "         6405,     9,   733,   719,  8148,    15,  4271,  1492,    13,     5,\n",
            "         2502,     9,     5,  6595,  7863,    13,  6231,     8,  8942, 50141,\n",
            "         1640,   134,   238,     8,    11,  1989,  6776,   204,  1640,   134,\n",
            "           43, 25991,     6, 50118, 45210,    35, 50118,  1640,   134,    43,\n",
            "        18912,    36,  3586,    43,   440,   155, 27996,    73,  6405, 22533,\n",
            "          159,     6, 22918,     7,     5,  4258,     9,     5, 17609,  8208,\n",
            "         7268, 16908,   721,  3377,     6,     5,  8608, 23920,     5,  1463,\n",
            "        22347,     5,  2526,  3266,    13,  5058,    31,   371,   749,     6,\n",
            "           11,  2098,     9,     5,   785,     8,  5788, 14808, 12944,    11,\n",
            "            5, 40396, 44482,     4, 50118,  1640,   176,    43,    96,  6265,\n",
            "           19,     5,  1065,  8608,     6,     5,  2526,  6595,  3266,   531,\n",
            "           28,  4460,    23,     5,  1389,   278,    66,    11,     5, 40396,\n",
            "            7,    42, 18912,     6, 50118,   133,  2526,  6595,  3266,  4997,\n",
            "            7,    11,  6776,   204,     9, 18912,    36,  3586,    43,   440,\n",
            "          155, 27996,    73,  6405,  5658,    28,  4460,    25,  4658,    11,\n",
            "            5, 40396,   259,   560,     4, 50118,   713, 18912,  5658,  2914,\n",
            "           88,  1370,    15,   545,   494,  3503,     4, 50118,   713, 18912,\n",
            "         5658,    28, 17014,    11,    63, 20090,     8,  2024, 10404,    11,\n",
            "           70, 10153,   532,     4,     2,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1])\n",
            "Sample 1 - Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "Sample 2 - Input IDs: tensor([    0, 10370,   448, 41336,  1437, 16718, 36296, 50118,  1116,   753,\n",
            "          902,  4525, 50118,   424,  4345, 30300,  7994,    73, 28654,    73,\n",
            "          717,  3586, 36103,     5,  1853,  3497,     9,  1600,     7,  7581,\n",
            "            6,    77, 10345,    88,    63,  4284,  3451,    50,  2195,   785,\n",
            "            6,   780,  2195,   474,  7668,    13,   184,    12, 20629,   931,\n",
            "            9,  1402,  6231,  3451,  3833,    13, 15505, 50118,  1640, 19933,\n",
            "            5,  1859,  2788,    16, 12757,    43, 50118,  1640,  3248,    73,\n",
            "        17500,    73,   717,  3586,    43, 50118, 13354, 31299, 41336,  3243,\n",
            "         1941, 10353, 23075,  1889, 37275, 26824, 50118,     6, 50118, 15852,\n",
            "         6203,     7,     5, 20704, 10584,     5,   796,  4713,  2573,     6,\n",
            "        50118, 15852,  6203,     7,  1080, 41185,  6791,    73,  6478,    73,\n",
            "          717,  3586,     9,   733,   719, 14488,    15, 11775,  1797,   136,\n",
            "            5,  7740,    88,     5, 10153,   532,     9, 28340, 11190,     7,\n",
            "         3451,    50,  2195,   785,    36,   134,   238,    25,    94, 13522,\n",
            "           30, 41185,  8572,    73, 38671,    73,   717,  3586,    36,   176,\n",
            "          238,     8,    11,  1989,  6776,   504,    36,   176,    43, 25991,\n",
            "            6, 50118, 45210,     6, 22918,     7,  6776,   365,    36,   134,\n",
            "           43,     9, 41185,  6791,    73,  6478,    73,   717,  3586,     6,\n",
            "         3451,     6,  2195,   785,    50,    97,  8720,    32,    45,  2087,\n",
            "            6,    23,     5,    86,     9,    49,  7740,    88,     5,  4284,\n",
            "            9,    10, 10153,   331,    31,   277, 10153,   331,     6,     7,\n",
            "        42982,    50,  5165,  8941,     7,  2195,   474,  1797,     6,  4682,\n",
            "          147,  1286,    13,    11,    14, 41185,   131, 50118, 45210,     6,\n",
            "        22918,     7,  6776,   504,    36,   176,    43,     9, 41185,  6791,\n",
            "           73,  6478,    73,   717,  3586,     6,     5, 10153,   532,   189,\n",
            "           28,  8672,     7,  7581,     6,    77, 10345,    88,    49,  4284,\n",
            "         3451,    50,  2195,   785,     6,   780,   474,  7668,    11,    98,\n",
            "          444,    25,   215,  1797,    32,    67,  4976,   159,    13,   184,\n",
            "           12, 20629,   931,   131, 50118, 45210,     5,  1853,  3497,     9,\n",
            "         1600,    30,     5,   128, 21119,  3109,   282,  1545,   992,   710,\n",
            "          163, 16146, 37655,   506,  1545,  5689,  9541, 14570, 40081, 24212,\n",
            "          225,  4356, 38240,   428,  1180,   108,     9,   973,   550, 14428,\n",
            "           36,   246,   238,    25,    94, 13522,    30,   128, 21119,  3109,\n",
            "          282,  1545,   108,     9,   112,   719, 10206,    36,   306,   238,\n",
            "           34,  2942,   780,   474,  1797,    13,     5,  2474,     9,  1402,\n",
            "         6231,  3451,  3833,    13, 15505,   131, 50118, 45210,    30, 30300,\n",
            "         7994,    73, 28654,    73,   717,  3586,    36,   245,   238,    25,\n",
            "           94, 13522,    30, 30300,  8572,    73, 29794,    73,   717,  3586,\n",
            "           36,   401,   238,     5,  1463,  8672,     5,  1853,  3497,     9,\n",
            "         1600,     7,  3253,     5,  1797,    67,     7,   785,  2942,    88,\n",
            "           63,  4284,    31,    97, 10153,   532,   131, 50118, 45210,    42,\n",
            "        19234,    21,  4159,    15,    10,  4667,  1453,    13,    10,   675,\n",
            "         4553,  9798,    15,  1105,   719, 10206,     6,    53,  2087,     7,\n",
            "           10,   678,  5064,     6,  5319,     5,  8515,     9,    10,  2573,\n",
            "        12930,  3552,    13,  6231,    12, 19520, 31464,  1295,  1468,   131,\n",
            "        50118, 45210,     5,  2573, 12930,  3552,    34,    45,   648,    57,\n",
            "         6533,   131,  9641,     5,    97,  4215,    61, 14267,    42, 19234,\n",
            "           33,    45,  1714,   131, 50118, 45210,  3891,     5, 19234,   197,\n",
            "           28,  3112,    13,    10,   617,   675,   131, 50118, 45210,     5,\n",
            "         1797,  1286,    13,    11,    42, 30300,    32,    11, 10753,    19,\n",
            "            5,  2979,     9,     5, 15125,  1674,    15, 12124,  1309,     6,\n",
            "        50118,     2])\n",
            "Sample 2 - Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "Sample 3 - Input IDs: tensor([    0,   245,     4,   398,     4, 44423, 13245, 13485,  3642,     9,\n",
            "            5,   796, 19818,   226, 30858,    73,  1646, 50118, 10370,   448,\n",
            "        41336, 26747,  6597,  6034,    36,   717,  3586,    43,   440,   504,\n",
            "         2546,    73,  4718, 50118,  1116,   204,   830, 15386, 50118,   462,\n",
            "        11918,   159,     6,    11,  2098,     9,     5,  6829,     8,  5030,\n",
            "          337,  1293,     6,   780,  1492,    13,     5,  2502,     9, 18912,\n",
            "           36,   717,  3586,    43,   440,   158,  3118,    73,  4718,    15,\n",
            "            5,  3521,     8,  2079,     9,   785,  2162,    11,    30,    41,\n",
            "         6530,  1218, 50118, 13354, 31299, 41336,  3243,  1941, 10353, 23075,\n",
            "         1889, 37275, 26824, 50118,     6, 50118, 15852,  6203,     7,     5,\n",
            "        20704, 10584,     5,   796,  4713,  2573,     6, 50118, 15852,  6203,\n",
            "            7,  1080, 18912,    36,   717,  3586,    43,   440,   158,  3118,\n",
            "           73,  4718,     9,   601,   392, 15386,    15,     5,  3521,     8,\n",
            "         2079,     9,   785,  2162,    11,    30,    41,  6530,  1218, 50141,\n",
            "         1640,   134,   238,     8,    11,  1989,  6776,   204,    36,   401,\n",
            "           43, 25991,     6, 50118, 45210,  1463, 18912,    36,   717,  3586,\n",
            "           43,   440,   601,  2036,    73,  4718,     9,   971,   550, 15386,\n",
            "        50141,  1640,   176,    43,  4976,   159,  1537,  1492,    13,     5,\n",
            "         2502,     9, 18912,    36,   717,  3586,    43,   440,   158,  3118,\n",
            "           73,  4718,    15,     5,  3521,     8,  2079,     9,   785,  2162,\n",
            "           11,    30,    41,  6530,  1218,   131,  9641,   780,  1492,    13,\n",
            "            5,  2502,     9, 18912,    36,   717,  3586,    43,   440,   158,\n",
            "         3118,    73,  4718,   197,    28,  5091,    11,  2098,     9,     5,\n",
            "         6829,     8,  5030,   337,  1293,   131, 50118, 45210,  6776,   155,\n",
            "           36,   176,    43,     9, 18912,    36,   717,  3586,    43,   440,\n",
            "          158,  3118,    73,  4718, 22533,   159,    14,   785,    61,    32,\n",
            "          547,    30,    41,  6530,  1218,   751,     5,  4284,     9,     5,\n",
            "        10153,   331,   624,  1060, 10542,    24,  5712,     8,    61,    32,\n",
            "           45,  1146,   124,    88,    14, 10153,   331,    32,     7,    28,\n",
            "        29616,     9,    23,     5,   850,     8,  2087,     7,     5,  1274,\n",
            "         4976,   159,    50,     7,    28,  4976,   159,    13,     5,   317,\n",
            "            9,  3521,   131,  9641,     6,   147,     5,   317,     9,  3521,\n",
            "           16, 17949,    15,     5,  4284,     9,   277, 10153,   331,     6,\n",
            "           24,    16,  2139,     7,   185,    88,  1316,     6,    77, 29770,\n",
            "            5,  2183,   425,    11,     5,   403,     9,    10,  1392,    23,\n",
            "           10,   425,  4460,    11,  3316,     8,    77, 15435,     5,  3527,\n",
            "          425,    11,     5,   403,     9,    10,  1392,    30,  8780,     6,\n",
            "          143,  5775,    50,   899,  1499, 29281,  5257,  5353, 28486,    50,\n",
            "         4159,    11,  2098,     9,   721,    11,    41, 14085,  1152,   227,\n",
            "            5, 10153,   532,    11,   864,   131, 50118, 45210,     6,   147,\n",
            "            5,   317,     9,  3521,    16, 17949,   751,     5,  2573,     6,\n",
            "         1316,   197,    28,   551,     9,   143, 12173,  4159,   131, 50118,\n",
            "        45210,    24,    16,  2139,     7, 27673,  1402,  7668,     9,  6776,\n",
            "          361,    36,   246,    43,     9,  1463, 18912,    36,   717,  3586,\n",
            "           43,   440, 29021,    73,  2545,     9,   601,   644, 14873, 11963,\n",
            "          159,  1537,  4271,  1492,    13,     5,  2502,     9,     5,   467,\n",
            "            9,  6595,     8,  5376, 23499,     8,  3316, 15435, 15588,    13,\n",
            "         7119,   785, 50141,  1640,   246,   238,    25,    94, 13522,    30,\n",
            "        18912,    36,   717,  3586,    43,   440,   501,  3083,    73,  4718,\n",
            "        50141,  1640,   306,  4397, 50118, 45210,     6,    13,  6216,     9,\n",
            "            5,  6397,     9,  6461,    14,     5,   785, 10696,    11,    10,\n",
            "          786,     2])\n",
            "Sample 3 - Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "\n",
            "Sample 4 - Input IDs: tensor([    0,   347,  5061,  6905,  3063,  1437, 16718, 36296, 50118,  1116,\n",
            "          883,   550, 11724, 50118,  3865, 41770,    10,  5883,     7,     5,\n",
            "          796, 17801,     8,  8114,  2573,    31,     5,   937,  1229,     9,\n",
            "            5,   796, 19818, 50118,  1640,  6232,    73, 35324,    73,   717,\n",
            "         3586,    43, 50118, 13354,   230,  5061,  6905,  3063,  3243,  1941,\n",
            "        10353, 23075,  1889, 50118, 10370,   448,  4154, 26824, 50118,     6,\n",
            "        50118, 15852,  6203,     7,     5, 20704, 10584,     5,   796,  4713,\n",
            "         2573,     6,     8,    11,  1989,  6776, 27089, 25991,     6, 50118,\n",
            "        15852,  6203,     7,     5,  2570,    31,     5,  1463,     6, 50118,\n",
            "        15852,  6203,     7,     5,  2979,     9,     5,   796,  3879,    36,\n",
            "          134,   238, 50118, 15852,  6203,     7,     5,  2979,     9,     5,\n",
            "         4713,     8,  3574,  1674,    36,   176,   238, 50118, 45210,     6,\n",
            "           23,    63,   529,    15,   820,   392, 11724,     6,     5,  1080,\n",
            "         1474,    14,  2705, 12174,    32,    41,  4499,  7510,     9,  2573,\n",
            "         1007,  1860,   131, 50118, 45210,     5,  2573,  4051,   539,    16,\n",
            "          145, 29142,  4075,     8,  2297,  1538,   131, 50118, 45210,    42,\n",
            "         8183,     8, 30325,    40, 16561,   483,     7,  9297,   633,  2687,\n",
            "          131,  9641,   323,  1797,   197,    28, 24012,  4628,     7, 19893,\n",
            "            5,  4914,     9,   209,   633,  2687,   131,  9641,    24,    16,\n",
            "         3891,  2139,     7,  3253,  6776,  4772,    36,   176,    43,    36,\n",
            "          428,    43,     9,     5, 20704, 10584,     5,   796, 17801,     8,\n",
            "         8114,  2573,   131, 50118, 45210,     6,    11,     5,  1455,  4215,\n",
            "            6,     5,  1915,  1286,    13,    30,     5, 11270,  3632, 20704,\n",
            "           32,    45,  7719,     7,  2879,   209,  1797,   131, 50118, 45210,\n",
            "            5,  5929,  3038,     9,    42,  1068,     6,   114,    45, 22104,\n",
            "         2550,     6,    74,    28, 19145,     7, 29223,   877, 14299,     5,\n",
            "         2573,    18,   937,  4042,  1068,     8,     7, 29210,     5, 21040,\n",
            "         6514,   709,     9,   776,  1713,     6,     8,    42,    74, 10115,\n",
            "            5,  8312,     9,    65,     9,     5,  2573,    18,  1049, 10366,\n",
            "          131, 50118, 45210,     5,   592,  1797, 24012,  4628,    40,    28,\n",
            "          716,     6,    15,     5,    65,   865,     6,    15,     5,   346,\n",
            "            9,  1315,   685,    81,    10,   576,   675,     8,     6,    15,\n",
            "            5,    97,   865,     6,    15,     5,   672,     9,   592, 12187,\n",
            "        15050,    30,   349, 10153,   331,    11,  2098,     9,   349,   633,\n",
            "          685,   131, 50118, 45210,    41, 30953,     9,  1191,   153, 11270,\n",
            "          791,    34,    57,  2867,    11,  8749,   158,   321,     9,     5,\n",
            "          937,  1229,     9,     5,   796, 19818,    13, 11724,     6, 50118,\n",
            "         4688,  9297,  5883,     9,  1191,   153, 11270,   791,    16, 28791,\n",
            "         4159,     7,     5,   796, 17801,     8,  8114,  2573,     6,    13,\n",
            "            5,   613,    76, 11724,     6,    66,     9,     5,   937,  1229,\n",
            "            9,     5,   796, 19818,    13,     5,   276,   613,    76,     6,\n",
            "            7,  1306,  2573,  5200,     9,   592,  1797,    11,  5976,     9,\n",
            "            5,  4051,   539,     6,    11, 10753,    19,     5,  4249, 15219,\n",
            "            9,     5, 11270,  3632, 20704,     4,     2,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1])\n",
            "Sample 4 - Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "Sample 5 - Input IDs: tensor([    0, 44503, 18912,    36,  3586,    43,   440,  7694,    73, 35153,\n",
            "        50118,  1116,   601,   644,  4999, 50118, 23032,   154,     5,  3527,\n",
            "         2183,   850,    13,  9050,     8,     5,  4532,  2887,    13,  6353,\n",
            "            6,  9050,     8, 15450,  9050,    13,     5, 11571,   212,  1736,\n",
            "        10021,     7,  8780,   223,     5,  2934, 10021,     7,  8780,  1286,\n",
            "           13,    11, 18912,    36,  3586,    43,   440,   564,  5339,    73,\n",
            "         6750, 50118, 13354, 31299, 41336,  3243,  1941, 10353, 23075,  1889,\n",
            "        37275, 26824, 50118,     6, 50118, 15852,  6203,     7,     5, 20704,\n",
            "        10584,     5,   796,  2573,     6, 50118, 15852,  6203,     7,  1080,\n",
            "        18912,    36,  3586,    43,   440,   316,  3118,    73, 37446,     9,\n",
            "          601,   392,  6193,    15,     5,  1537,  6010,     9,     5,   210,\n",
            "           11,  5803,     8,  5803,   785,  1640,   134,   238,    25,    94,\n",
            "        13522,    30,  1463, 18912,    36,  3586,    43,   440,   654,   466,\n",
            "           73, 31099,  1640,   176,   238,     8,    11,  1989,  6776,   158,\n",
            "        25991,     6, 50118, 45210,    35, 50118,  1640,   134,    43,    20,\n",
            "         6530,  2244,    32,     6, 22918,     7,  1463, 18912,    36,  3586,\n",
            "           43,   440,   564,  5339,    73,  6750,     9,   379,   719,  7528,\n",
            "           15,     5,  1392,     9,  9050,    23,  2906,   850,     8,     5,\n",
            "        18379,     9,  2887,    13,  6353,     6,  9050,     8, 15450,  9050,\n",
            "           13,   304,    11,     5, 15030,     9, 29166,   785,     6,  2480,\n",
            "           12, 34806,     8,    97,   689,   620, 22902,  1640,   246,   238,\n",
            "           25,    94, 13522,    30, 18912,    36,  3586,    43,   440,   231,\n",
            "         2022,    73, 17472,  1640,   306,   238,     7,  1331,    30, 10021,\n",
            "            7,  8780,  1402, 20631,     9,  9050,    14,    51,   946,     8,\n",
            "            7,  4470,  2887,    13,  6353,     6,  9050,     8, 15450,  9050,\n",
            "            4,  6776,   504,     9,    14, 18912, 14808, 15719,    14,    11,\n",
            "            5,  1109,     9,     5,  3805,   268,   829,    11,  1263,     7,\n",
            "          349,  1736, 10021,     7,  8780,    10,  3527,  2183,   425,  5658,\n",
            "           28,  4460,    13,  9050,     8,  4532,  2887,  5658,    28,  4460,\n",
            "           13,  6353,     6,  9050,     8, 15450,  9050,     4,    85,    16,\n",
            "          617, 14808, 12944,    14,     5,   425,    50,  2887,   189, 10104,\n",
            "          309,     7,     5,  3833,   304,     9,     5,  9050,     6,    63,\n",
            "         5886,  1383,     8,     5, 40363,  7089,     6,     8,    14,    10,\n",
            "          568,   189,    67,    28,   551,     7,   146,   117,  2354,    11,\n",
            "         1263,     7,     5,  3805,   268,  4813,     4,    20,  1280,  1640,\n",
            "           29,    43,     9,     5,  5774,  5157,   531,    28,  4460, 14649,\n",
            "            4, 50118,  1640,   176,    43,    20,  1797,  1286,    13,    11,\n",
            "           42, 18912,    32,    11, 10753,    19,     5,  2979,     9,     5,\n",
            "         1753,  1674,    13, 22124,     8, 22124,  9467,     6, 50118,   133,\n",
            "         3527,  2183,   850,     8,     5,  4532,  2887,     8,  5774,  5157,\n",
            "         9889,    13,     5, 11571,   212,  1736, 10021,     7,  8780,     6,\n",
            "          223,     5,  2934, 10021,     7,  8780,  1286,    13,    11, 18912,\n",
            "           36,  3586,    43,   440,   564,  5339,    73,  6750,     6,  5658,\n",
            "           28,  4460,    25,  4658,    11,     5, 40396,   259,   560,     4,\n",
            "        50118,   713, 18912,  5658,  2914,    88,  1370,    15,   504,   644,\n",
            "         4999,     4, 50118,   713, 18912,  5658,    28, 17014,    11,    63,\n",
            "        20090,     8,  2024, 10404,    11,    70, 10153,   532,     4,     2,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1])\n",
            "Sample 5 - Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming tokenized_data contains your tokenized data\n",
        "for i in range(5):  # Print the first 5 tokenized samples\n",
        "    print(f\"Sample {i + 1} - Input IDs: {tokenized_data['input_ids'][i]}\")\n",
        "    print(f\"Sample {i + 1} - Attention Mask: {tokenized_data['attention_mask'][i]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfhJRpVB9ANg"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch Dataset using CustomDataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1WkrJVG-glq"
      },
      "outputs": [],
      "source": [
        "# Create PyTorch Dataset using CustomDataset class\n",
        "custom_dataset = CustomDataset(tokenized_data[\"input_ids\"], tokenized_data[\"attention_mask\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxXO7B0AEyx2"
      },
      "outputs": [],
      "source": [
        "# Define your batch size\n",
        "your_batch_size = 16\n",
        "learning_rate = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDWycMIO-1P5"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader\n",
        "data_loader = DataLoader(custom_dataset, batch_size=your_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXH7La1T-1M-"
      },
      "outputs": [],
      "source": [
        "# Define Luke model configuration and load the pre-trained LukeForMaskedLM model\n",
        "config = LukeConfig.from_pretrained(\"studio-ousia/luke-base\")\n",
        "model = LukeForMaskedLM.from_pretrained(\"studio-ousia/luke-base\", config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaAbIAacIGdk",
        "outputId": "023b913f-05a2-488e-f7e9-843d0b1bc0f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LukeForMaskedLM(\n",
              "  (luke): LukeModel(\n",
              "    (embeddings): LukeEmbeddings(\n",
              "      (word_embeddings): Embedding(50267, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (entity_embeddings): LukeEntityEmbeddings(\n",
              "      (entity_embeddings): Embedding(500000, 256, padding_idx=0)\n",
              "      (entity_embedding_dense): Linear(in_features=256, out_features=768, bias=False)\n",
              "      (position_embeddings): Embedding(514, 768)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LukeEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x LukeLayer(\n",
              "          (attention): LukeAttention(\n",
              "            (self): LukeSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (w2e_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (e2w_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (e2e_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LukeSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LukeIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LukeOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): LukePooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): LukeLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=50267, bias=True)\n",
              "  )\n",
              "  (entity_predictions): EntityPredictionHead(\n",
              "    (transform): EntityPredictionHeadTransform(\n",
              "      (dense): Linear(in_features=768, out_features=256, bias=True)\n",
              "      (transform_act_fn): GELUActivation()\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): Linear(in_features=256, out_features=500000, bias=False)\n",
              "  )\n",
              "  (loss_fn): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Load the previously saved checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/NLP/luke_checkpoints/luke_model_checkpoint_epoch_11.pth\"\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# Load the model state_dict\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heRABPhxAZAf",
        "outputId": "b3a8b10a-3503-4fcb-a757-95091f6a8153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MebTU7i2EaBQ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 11 # Set the number of epochs 40\n",
        "mask_probability = 0.15\n",
        "accumulation_steps = 4  # Define the accumulation steps\n",
        "checkpoint_dir = \"/content/drive/MyDrive/NLP/luke_checkpoints\"  # Specify the directory to save the checkpoints\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    for batch_idx, batch in enumerate(tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        inputs = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "\n",
        "        mask_indices = torch.rand(inputs.shape) < mask_probability\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        inputs[mask_indices] = tokenizer.mask_token_id\n",
        "\n",
        "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    # Save model checkpoint after each epoch with epoch number in the file name\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"luke_model_checkpoint_epoch_{epoch + 1}.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    # Save the further pretrained model\n",
        "    model.save_pretrained(\"/content/drive/MyDrive/NLP/luke_eurlex_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e0LJR0BTceo"
      },
      "outputs": [],
      "source": [
        "#To later load the pretrained Luke model for downstream tasks\n",
        "\n",
        "from transformers import LukeForMaskedLM, LukeTokenizer\n",
        "\n",
        "# Load LukeTokenizer\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-base\")\n",
        "\n",
        "# Load the saved model\n",
        "model = LukeForMaskedLM.from_pretrained(\"/content/drive/MyDrive/NLP/luke_eurlex_model\")\n",
        "\n",
        "# Now you can use the loaded model for downstream tasks\n",
        "# For example, you can use it for text generation, classification, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Evekxl6TcSW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}