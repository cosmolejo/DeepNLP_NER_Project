{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYGOkEX0NFDx",
        "outputId": "c076035d-10b1-4f70-b00e-3b8321ac2b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV7kP-Fh5Luf",
        "outputId": "e353cc9d-32a6-49ce-c4d2-084011deace2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair) (1.34.48)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.3.4)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair) (6.1.3)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.7.3)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.20.3)\n",
            "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.4)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2023.12.25)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.2)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.3.3)\n",
            "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.37.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (1.26.18)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair) (0.6.0)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (3.0.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.48 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.34.48)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.1.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.27.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "5w1uhuuU447G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_conll(dataset):\n",
        "    conll_data = []\n",
        "\n",
        "    for entry in dataset:\n",
        "        text = entry['data']['text']\n",
        "        annotations = entry['annotations'][0]['result']\n",
        "\n",
        "        entities = []\n",
        "        for annotation in annotations:\n",
        "            entity_text = annotation['value']['text']\n",
        "            start = annotation['value']['start']\n",
        "            end = annotation['value']['end']\n",
        "            label = annotation['value']['labels'][0]\n",
        "            entities.append((start, end, label))\n",
        "\n",
        "        # Sort entities by start index\n",
        "        entities.sort(key=lambda x: x[0])\n",
        "\n",
        "        # Generate CoNLL format\n",
        "        conll_lines = []\n",
        "        current_index = 0\n",
        "        for start, end, label in entities:\n",
        "            token = text[current_index:start].strip()\n",
        "            if token:\n",
        "                token_lines = [(token, 'O')] * len(token.split())\n",
        "                conll_lines.extend(token_lines)\n",
        "\n",
        "            entity_text = text[start:end]\n",
        "            entity_tokens = entity_text.strip().split()\n",
        "            if len(entity_tokens) == 1:\n",
        "                conll_lines.append((entity_tokens[0], 'B-' + label))\n",
        "            else:\n",
        "                conll_lines.append((entity_tokens[0], 'B-' + label))\n",
        "                for token in entity_tokens[1:]:\n",
        "                    conll_lines.append((token, 'I-' + label))\n",
        "\n",
        "            current_index = end\n",
        "\n",
        "        # Handle remaining tokens\n",
        "        remaining_text = text[current_index:].strip()\n",
        "        if remaining_text:\n",
        "            remaining_tokens = remaining_text.split()\n",
        "            remaining_lines = [(token, 'O') for token in remaining_tokens]\n",
        "            conll_lines.extend(remaining_lines)\n",
        "\n",
        "        conll_data.append(conll_lines)\n",
        "\n",
        "    return conll_data"
      ],
      "metadata": {
        "id": "SmNq0OgkbWgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataset from JSON file\n",
        "with open('/content/drive/MyDrive/NLP/NER_TRAIN/NER_TRAIN_ALL.json', 'r') as file:\n",
        "    train_data = json.load(file)\n",
        "\n",
        "# Read dataset from JSON file\n",
        "with open('/content/drive/MyDrive/NLP/NER_DEV/NER_DEV_ALL.json', 'r') as file:\n",
        "    test_data = json.load(file)"
      ],
      "metadata": {
        "id": "sDWVz0gqbzFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to CoNLL format\n",
        "train_conll_format = convert_to_conll(train_data)\n",
        "test_conll_format = convert_to_conll(test_data)"
      ],
      "metadata": {
        "id": "l8J8zuM3bZMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flair"
      ],
      "metadata": {
        "id": "ZLiBaORk5I63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.training_utils import EvaluationMetric\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "TAaQlZr8fZKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Flair tag type\n",
        "tag_type = 'ner'"
      ],
      "metadata": {
        "id": "hEZiyOiifZIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Dictionary\n",
        "\n",
        "# Define the labels\n",
        "original_label_list = [\n",
        "    \"COURT\",\n",
        "    \"PETITIONER\",\n",
        "    \"RESPONDENT\",\n",
        "    \"JUDGE\",\n",
        "    \"DATE\",\n",
        "    \"ORG\",\n",
        "    \"GPE\",\n",
        "    \"STATUTE\",\n",
        "    \"PROVISION\",\n",
        "    \"PRECEDENT\",\n",
        "    \"CASE_NUMBER\",\n",
        "    \"WITNESS\",\n",
        "    \"OTHER_PERSON\",\n",
        "    \"LAWYER\"\n",
        "]\n",
        "\n",
        "# Create a Dictionary object\n",
        "tag_dictionary = Dictionary()\n",
        "for label in original_label_list:\n",
        "    tag_dictionary.add_item(label)\n",
        "\n",
        "# Add 'O' label\n",
        "tag_dictionary.add_item('O')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVhIvutGfZFN",
        "outputId": "10449fb7-2687-40c8-ca79-51008ab5ec82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embeddings\n",
        "embedding_types = [\n",
        "    WordEmbeddings('glove'),\n",
        "    FlairEmbeddings('news-forward'),\n",
        "    FlairEmbeddings('news-backward')\n",
        "]\n",
        "embeddings = StackedEmbeddings(embeddings=embedding_types)\n"
      ],
      "metadata": {
        "id": "i3BVpGILh4k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize sequence tagger\n",
        "tagger = SequenceTagger(hidden_size=256,\n",
        "                        embeddings=embeddings,\n",
        "                        tag_dictionary=tag_dictionary,\n",
        "                        tag_type=tag_type)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glNhMEGth9TI",
        "outputId": "675619e9-c740-4b55-ae23-5ed483e92674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-23 11:07:52,045 SequenceTagger predicts: Dictionary with 16 tags: <unk>, COURT, PETITIONER, RESPONDENT, JUDGE, DATE, ORG, GPE, STATUTE, PROVISION, PRECEDENT, CASE_NUMBER, WITNESS, OTHER_PERSON, LAWYER, O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Corpus, Sentence\n",
        "from flair.datasets import CONLL_03\n",
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
        "\n",
        "# Convert your train and test datasets into Flair Sentence objects\n",
        "train_sentences = [Sentence(' '.join([token[0] for token in sentence])) for sentence in train_conll_format]\n",
        "test_sentences = [Sentence(' '.join([token[0] for token in sentence])) for sentence in test_conll_format]\n",
        "\n",
        "# Convert your train and test datasets into Flair Sentence objects with entity annotations\n",
        "train_annotations = [[(token[0], token[1]) for token in sentence] for sentence in train_conll_format]\n",
        "test_annotations = [[(token[0], token[1]) for token in sentence] for sentence in test_conll_format]\n",
        "\n",
        "# Initialize Flair corpus\n",
        "corpus = Corpus(train=train_sentences, test=test_sentences)\n",
        "\n",
        "# Add entity annotations to corpus\n",
        "for sentence, annotations in zip(corpus.train, train_annotations):\n",
        "    for token, label in annotations:\n",
        "        sentence.add_label('ner', label)\n",
        "\n",
        "for sentence, annotations in zip(corpus.test, test_annotations):\n",
        "    for token, label in annotations:\n",
        "        sentence.add_label('ner', label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJIiADVxiFOn",
        "outputId": "2aaf99bb-d7cf-44d4-a99a-10c656ec41bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-23 11:10:45,134 No dev split found. Using 0% (i.e. 1100 samples) of the train split as dev data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize trainer\n",
        "trainer = ModelTrainer(tagger, corpus)\n"
      ],
      "metadata": {
        "id": "uYp0zXwzkbUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train('/content/drive/MyDrive/NLP/resources/taggers/ner-english',\n",
        "              train_with_dev=True,\n",
        "              mini_batch_size=1,\n",
        "              max_epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l23EAyjukkpm",
        "outputId": "4c3489c3-a4da-4228-c524-68dc0f3c1833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-23 11:10:45,801 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,803 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings(\n",
            "      'glove'\n",
            "      (embedding): Embedding(400001, 100)\n",
            "    )\n",
            "    (list_embedding_1): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "      )\n",
            "    )\n",
            "    (list_embedding_2): FlairEmbeddings(\n",
            "      (lm): LanguageModel(\n",
            "        (drop): Dropout(p=0.05, inplace=False)\n",
            "        (encoder): Embedding(300, 100)\n",
            "        (rnn): LSTM(100, 2048)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)\n",
            "  (rnn): LSTM(4196, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=18, bias=True)\n",
            "  (loss_function): ViterbiLoss()\n",
            "  (crf): CRF()\n",
            ")\"\n",
            "2024-02-23 11:10:45,805 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,807 Corpus: 9895 train + 1100 dev + 1074 test sentences\n",
            "2024-02-23 11:10:45,808 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,809 Train:  10995 sentences\n",
            "2024-02-23 11:10:45,811         (train_with_dev=True, train_with_test=False)\n",
            "2024-02-23 11:10:45,812 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,813 Training Params:\n",
            "2024-02-23 11:10:45,815  - learning_rate: \"0.1\" \n",
            "2024-02-23 11:10:45,816  - mini_batch_size: \"1\"\n",
            "2024-02-23 11:10:45,817  - max_epochs: \"150\"\n",
            "2024-02-23 11:10:45,818  - shuffle: \"True\"\n",
            "2024-02-23 11:10:45,820 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,821 Plugins:\n",
            "2024-02-23 11:10:45,822  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
            "2024-02-23 11:10:45,824 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,825 Final evaluation on model from best epoch (best-model.pt)\n",
            "2024-02-23 11:10:45,827  - metric: \"('micro avg', 'f1-score')\"\n",
            "2024-02-23 11:10:45,828 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,829 Computation:\n",
            "2024-02-23 11:10:45,831  - compute on device: cpu\n",
            "2024-02-23 11:10:45,833  - embedding storage: cpu\n",
            "2024-02-23 11:10:45,834 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,836 Model training base path: \"/content/drive/MyDrive/NLP/resources/taggers/ner-english\"\n",
            "2024-02-23 11:10:45,838 ----------------------------------------------------------------------------------------------------\n",
            "2024-02-23 11:10:45,839 ----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, data):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for sentence in data:\n",
        "        text = ' '.join([token[0] for token in sentence])\n",
        "        true_label = [token[1] for token in sentence]\n",
        "\n",
        "        # Create Flair sentence\n",
        "        flair_sentence = Sentence(text)\n",
        "\n",
        "        # Predict NER tags\n",
        "        model.predict(flair_sentence)\n",
        "        predicted_labels = [entity.tag for entity in flair_sentence.get_spans('ner')]\n",
        "\n",
        "        predictions.append(predicted_labels)\n",
        "        true_labels.append(true_label)\n",
        "\n",
        "    # Convert labels to numpy arrays for computation\n",
        "    predictions = np.array(predictions)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(predictions, true_labels)\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "mGsKiIn3kphz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predictions, true_labels):\n",
        "    # Flatten the predictions and true labels\n",
        "    pred_flat = predictions.flatten()\n",
        "    true_flat = true_labels.flatten()\n",
        "\n",
        "    # Remove 'O' labels\n",
        "    pred_flat_filtered = pred_flat[pred_flat != 'O']\n",
        "    true_flat_filtered = true_flat[true_flat != 'O']\n",
        "\n",
        "    # Calculate true positive, false positive, and false negative counts for each label\n",
        "    tp_counts = {}\n",
        "    fp_counts = {}\n",
        "    fn_counts = {}\n",
        "\n",
        "    for label in np.unique(true_flat_filtered):\n",
        "        tp_counts[label] = np.sum((true_flat_filtered == label) & (pred_flat_filtered == label))\n",
        "        fp_counts[label] = np.sum((true_flat_filtered != label) & (pred_flat_filtered == label))\n",
        "        fn_counts[label] = np.sum((true_flat_filtered == label) & (pred_flat_filtered != label))\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for each label\n",
        "    label_metrics = {}\n",
        "    for label in np.unique(true_flat_filtered):\n",
        "        precision = tp_counts[label] / (tp_counts[label] + fp_counts[label] + 1e-9)\n",
        "        recall = tp_counts[label] / (tp_counts[label] + fn_counts[label] + 1e-9)\n",
        "        f1_score = 2 * precision * recall / (precision + recall + 1e-9)\n",
        "        label_metrics[label] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1_score}\n",
        "\n",
        "    # Calculate weighted average of F1-scores\n",
        "    total_instances = len(true_flat_filtered)\n",
        "    weighted_f1_score = np.sum([label_metrics[label][\"f1-score\"] * np.sum(true_flat_filtered == label) / total_instances\n",
        "                                for label in label_metrics])\n",
        "\n",
        "    return {\n",
        "        \"f1-type-match\": weighted_f1_score,\n",
        "        \"f1-partial\": np.mean([label_metrics[label][\"f1-score\"] for label in label_metrics]),\n",
        "        \"f1-strict\": np.sum([tp_counts[label] for label in label_metrics]) / np.sum([tp_counts[label] + fn_counts[label] for label in label_metrics]),\n",
        "        \"f1-exact\": np.sum([tp_counts[label] for label in label_metrics]) / total_instances\n",
        "    }"
      ],
      "metadata": {
        "id": "jYLwHHt4kvp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "evaluation_metrics = evaluate_model(tagger, test_conll_format)\n",
        "\n",
        "# Print or use the evaluation metrics as needed\n",
        "print(\"Evaluation Metrics:\", evaluation_metrics)"
      ],
      "metadata": {
        "id": "Crx5Z4T4RL6B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}